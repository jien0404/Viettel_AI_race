import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from typing import List, Dict, Any
import os
import base64

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

class ContextEnricher:
    def __init__(self, model_name: str = "Qwen/Qwen3-VL-4B-Thinking"):
        print(f"ƒêang t·∫£i m√¥ h√¨nh Vision-Language: {model_name}...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            dtype="auto",
            device_map="auto"
        )

        # üëá S·ª≠a ·ªü ƒë√¢y
        self.processor = AutoProcessor.from_pretrained(model_name)
        if hasattr(self.processor, "tokenizer"):
            self.processor.tokenizer.padding_side = "left"
            if self.processor.tokenizer.pad_token is None:
                self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token

        print(f"T·∫£i m√¥ h√¨nh ho√†n t·∫•t. S·ª≠ d·ª•ng thi·∫øt b·ªã: {self.device}")

    # S·ª≠a ƒë·ªïi h√†m generate ƒë·ªÉ x·ª≠ l√Ω batch
    def _generate_batch_response(self, messages_batch: List[List[Dict]]) -> List[str]:
        try:
            # `apply_chat_template` c√≥ th·ªÉ x·ª≠ l√Ω m·ªôt list c√°c conversation
            inputs = self.processor.apply_chat_template(
                messages_batch,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                padding=True, # Th√™m padding ƒë·ªÉ c√°c chu·ªói c√≥ c√πng ƒë·ªô d√†i
                return_tensors="pt"
            ).to(self.model.device)

            generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
            # Gi·∫£i m√£ k·∫øt qu·∫£ cho c·∫£ batch
            output_texts = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            # C·∫ßn lo·∫°i b·ªè ph·∫ßn prompt kh·ªèi output
            # ƒê√¢y l√† m·ªôt c√°ch x·ª≠ l√Ω ƒë∆°n gi·∫£n, c√≥ th·ªÉ c·∫ßn tinh ch·ªânh
            final_outputs = []
            for i, full_text in enumerate(output_texts):
                # L·∫•y l·∫°i prompt ƒë√£ ƒë∆∞·ª£c format
                prompt_text_only = self.processor.decode(inputs['input_ids'][i], skip_special_tokens=True)
                # Lo·∫°i b·ªè prompt kh·ªèi output
                if full_text.startswith(prompt_text_only):
                    final_outputs.append(full_text[len(prompt_text_only):].strip())
                else: # Fallback
                     final_outputs.append(full_text)
            
            return final_outputs

        except Exception as e:
            print(f"L·ªói trong qu√° tr√¨nh generate batch c·ªßa model: {e}")
            return ["[L·ªói x·ª≠ l√Ω]" for _ in messages_batch]

    # Vi·∫øt l·∫°i ho√†n to√†n h√†m enrich_chunks_with_llm ƒë·ªÉ s·ª≠ d·ª•ng batching
    def enrich_chunks_with_llm(self, chunks: List[Dict[str, Any]], 
                               doc_folder_path: str, 
                               batch_size: int = 4) -> List[Dict[str, Any]]:
        
        enriched_chunks_data = []
        
        # Gom c√°c chunk th√†nh c√°c l√¥ ƒë·ªÉ x·ª≠ l√Ω
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i+batch_size]
            print(f"  > ƒêang x·ª≠ l√Ω batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}...")

            messages_batch = []
            for chunk in batch_chunks:
                content_to_enrich = chunk.get('content_for_enrichment', '')
                chunk_type = chunk.get('chunk_type')

                # T·∫°o prompt cho t·ª´ng chunk trong batch
                if chunk_type == 'image_context':
                    image_path_relative = chunk['metadata'].get('image_path', '')
                    image_path_full = os.path.join(doc_folder_path, image_path_relative)
                    
                    if os.path.exists(image_path_full):
                        image_uri = f"data:image/jpeg;base64,{encode_image_to_base64(image_path_full)}"
                        prompt_text = (
                            "B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch h√¨nh ·∫£nh k·ªπ thu·∫≠t. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh vƒÉn b·∫£n xung quanh.\n"
                            "H√ÉY TR·∫¢ L·ªúI HO√ÄN TO√ÄN B·∫∞NG TI·∫æNG VI·ªÜT. B·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi ngay l·∫≠p t·ª©c, kh√¥ng d√πng l·ªùi ch√†o hay c√¢u d·∫´n.\n\n"
                            f"--- NG·ªÆ C·∫¢NH VƒÇN B·∫¢N ---\n{content_to_enrich}\n\n"
                            "--- Y√äU C·∫¶U ---\n"
                            "H√£y ph√¢n t√≠ch h√¨nh ·∫£nh v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë√∫ng c·∫•u tr√∫c Markdown sau:\n\n"
                            "### M√î T·∫¢ H√åNH ·∫¢NH\n"
                            "(M√¥ t·∫£ m·ªôt c√°ch kh√°ch quan v√† chi ti·∫øt t·∫•t c·∫£ c√°c ƒë·ªëi t∆∞·ª£ng, bi·ªÉu ƒë·ªì, v√† th√†nh ph·∫ßn trong ·∫£nh.)\n\n"
                            "### VƒÇN B·∫¢N TRONG ·∫¢NH (OCR)\n"
                            "(Tr√≠ch xu·∫•t TO√ÄN B·ªò vƒÉn b·∫£n, ch·ªØ s·ªë, k√Ω hi·ªáu c√≥ trong ·∫£nh, kh√¥ng b·ªè s√≥t. N·∫øu kh√¥ng c√≥, ghi r√µ 'Kh√¥ng c√≥ vƒÉn b·∫£n'.)\n\n"
                            "### PH√ÇN T√çCH & K·∫æT N·ªêI\n"
                            "(Di·ªÖn gi·∫£i √Ω nghƒ©a c·ªßa h√¨nh ·∫£nh v√† m·ªëi li√™n h·ªá c·ªßa n√≥ v·ªõi ng·ªØ c·∫£nh vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.)"
                        )
                        messages_batch.append([
                            {"role": "user", "content": [
                                {"type": "text", "text": prompt_text},
                                {"type": "image", "image": image_uri}
                            ]}
                        ])
                    else:
                        messages_batch.append([{"role": "user", "content": "File ·∫£nh kh√¥ng t·ªìn t·∫°i."}])

                elif chunk_type == 'text':
                    prompt_text = (
                        "B·∫°n l√† m·ªôt chuy√™n gia l√†m gi√†u d·ªØ li·ªáu cho h·ªá th·ªëng t√¨m ki·∫øm (search engine). Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch v√† m·ªü r·ªông ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y.\n"
                        "H√ÉY TR·∫¢ L·ªúI HO√ÄN TO√ÄN B·∫∞NG TI·∫æNG VI·ªÜT. B·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi ngay l·∫≠p t·ª©c, kh√¥ng d√πng c√°c c√¢u d·∫´n nh∆∞ 'D∆∞·ªõi ƒë√¢y l√† ph·∫ßn tr·∫£ l·ªùi...'\n\n"
                        f"--- D·ªÆ LI·ªÜU G·ªêC ---\n{content_to_enrich}\n\n"
                        "--- Y√äU C·∫¶U ---\n"
                        "H√£y t·∫°o ra m·ªôt phi√™n b·∫£n l√†m gi√†u c·ªßa d·ªØ li·ªáu g·ªëc theo ƒë√∫ng c·∫•u tr√∫c Markdown sau:\n\n"
                        "### T√ìM T·∫ÆT & M·ªû R·ªòNG\n"
                        "(T√≥m t·∫Øt l·∫°i √Ω ch√≠nh v√† di·ªÖn gi·∫£i, m·ªü r·ªông n√≥ v·ªõi c√°c chi ti·∫øt b·ªï sung ƒë·ªÉ l√†m r√µ ng·ªØ c·∫£nh.)\n\n"
                        "### T·ª™ KH√ìA CH√çNH\n"
                        "- (Li·ªát k√™ 5-7 t·ª´ kh√≥a quan tr·ªçng nh·∫•t, m·ªói t·ª´ kh√≥a m·ªôt d√≤ng)\n\n"
                        "### THU·∫¨T NG·ªÆ LI√äN QUAN\n"
                        "- (Li·ªát k√™ c√°c t·ª´ ƒë·ªìng nghƒ©a ho·∫∑c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t c√≥ li√™n quan ch·∫∑t ch·∫Ω.)\n\n"
                        "### C√ÇU H·ªéI TI·ªÄM NƒÇNG\n"
                        "- (T·∫°o ra 3 c√¢u h·ªèi m√† ƒëo·∫°n vƒÉn n√†y c√≥ th·ªÉ tr·∫£ l·ªùi tr·ª±c ti·∫øp.)"
                    )
                    messages_batch.append([{"role": "user", "content": [{"type": "text", "text": prompt_text}]}])
                else:
                    messages_batch.append([{"role": "user", "content": f"Lo·∫°i chunk kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£."}])
            
            # G·ªçi model ƒë·ªÉ x·ª≠ l√Ω c·∫£ batch
            enriched_contents = self._generate_batch_response(messages_batch)

            # G√°n k·∫øt qu·∫£ tr·∫£ v·ªÅ cho c√°c chunk t∆∞∆°ng ·ª©ng
            for j, chunk in enumerate(batch_chunks):
                enriched_chunks_data.append({
                    'doc_name': chunk['doc_name'],
                    'chunk_type': chunk['chunk_type'],
                    'enriched_content': enriched_contents[j],
                    'metadata': chunk['metadata']
                })
        
        return enriched_chunks_data