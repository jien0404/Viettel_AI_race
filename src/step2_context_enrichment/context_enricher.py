import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from typing import List, Dict, Any
import os
import base64

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

class ContextEnricher:
    def __init__(self, model_name: str = "Qwen/Qwen3-VL-4B-Instruct"):
        print(f"ƒêang t·∫£i m√¥ h√¨nh Vision-Language: {model_name}...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            dtype="auto",
            device_map="auto"
        )

        # üëá S·ª≠a ·ªü ƒë√¢y
        self.processor = AutoProcessor.from_pretrained(model_name)
        if hasattr(self.processor, "tokenizer"):
            self.processor.tokenizer.padding_side = "left"
            if self.processor.tokenizer.pad_token is None:
                self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token

        print(f"T·∫£i m√¥ h√¨nh ho√†n t·∫•t. S·ª≠ d·ª•ng thi·∫øt b·ªã: {self.device}")

    # S·ª≠a ƒë·ªïi h√†m generate ƒë·ªÉ x·ª≠ l√Ω batch
    def _generate_batch_response(self, messages_batch: List[List[Dict]]) -> List[str]:
        try:
            # `apply_chat_template` c√≥ th·ªÉ x·ª≠ l√Ω m·ªôt list c√°c conversation
            inputs = self.processor.apply_chat_template(
                messages_batch,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                padding=True, # Th√™m padding ƒë·ªÉ c√°c chu·ªói c√≥ c√πng ƒë·ªô d√†i
                return_tensors="pt"
            ).to(self.model.device)

            generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
            # Gi·∫£i m√£ k·∫øt qu·∫£ cho c·∫£ batch
            output_texts = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            # C·∫ßn lo·∫°i b·ªè ph·∫ßn prompt kh·ªèi output
            # ƒê√¢y l√† m·ªôt c√°ch x·ª≠ l√Ω ƒë∆°n gi·∫£n, c√≥ th·ªÉ c·∫ßn tinh ch·ªânh
            final_outputs = []
            for i, full_text in enumerate(output_texts):
                # L·∫•y l·∫°i prompt ƒë√£ ƒë∆∞·ª£c format
                prompt_text_only = self.processor.decode(inputs['input_ids'][i], skip_special_tokens=True)
                # Lo·∫°i b·ªè prompt kh·ªèi output
                if full_text.startswith(prompt_text_only):
                    final_outputs.append(full_text[len(prompt_text_only):].strip())
                else: # Fallback
                     final_outputs.append(full_text)
            
            return final_outputs

        except Exception as e:
            print(f"L·ªói trong qu√° tr√¨nh generate batch c·ªßa model: {e}")
            return ["[L·ªói x·ª≠ l√Ω]" for _ in messages_batch]

    # Vi·∫øt l·∫°i ho√†n to√†n h√†m enrich_chunks_with_llm ƒë·ªÉ s·ª≠ d·ª•ng batching
    def enrich_chunks_with_llm(self, chunks: List[Dict[str, Any]], 
                               doc_folder_path: str, 
                               batch_size: int = 4) -> List[Dict[str, Any]]:
        
        enriched_chunks_data = []
        
        # Gom c√°c chunk th√†nh c√°c l√¥ ƒë·ªÉ x·ª≠ l√Ω
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i+batch_size]
            print(f"  > ƒêang x·ª≠ l√Ω batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}...")

            messages_batch = []
            for chunk in batch_chunks:
                content_to_enrich = chunk.get('content_for_enrichment', '')
                chunk_type = chunk.get('chunk_type')

                # T·∫°o prompt cho t·ª´ng chunk trong batch
                if chunk_type == 'image_context':
                    image_path_relative = chunk['metadata'].get('image_path', '')
                    image_path_full = os.path.join(doc_folder_path, image_path_relative)
                    
                    if os.path.exists(image_path_full):
                        image_uri = f"data:image/jpeg;base64,{encode_image_to_base64(image_path_full)}"
                        prompt_text = (
                            "B·∫†N L√Ä TR·ª¢ L√ù AI. H√ÉY TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.\n\n"
                            f"**Ng·ªØ c·∫£nh:**\n{content_to_enrich}\n\n"
                            "**Y√äU C·∫¶U:** D·ª±a v√†o h√¨nh ·∫£nh v√† ng·ªØ c·∫£nh, h√£y:\n"
                            "1. M√¥ t·∫£ chi ti·∫øt h√¨nh ·∫£nh.\n"
                            "2. Tr√≠ch xu·∫•t (OCR) to√†n b·ªô vƒÉn b·∫£n trong ·∫£nh.\n"
                            "3. Di·ªÖn gi·∫£i √Ω nghƒ©a c·ªßa ·∫£nh."
                        )
                        messages_batch.append([
                            {"role": "user", "content": [
                                {"type": "text", "text": prompt_text},
                                {"type": "image", "image": image_uri}
                            ]}
                        ])
                    else:
                        messages_batch.append([{"role": "user", "content": "File ·∫£nh kh√¥ng t·ªìn t·∫°i."}])

                elif chunk_type == 'text':
                    prompt_text = (
                        "B·∫†N L√Ä TR·ª¢ L√ù AI. H√ÉY TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.\n\n"
                        f"**N·ªôi dung c·∫ßn l√†m gi√†u:**\n{content_to_enrich}\n\n"
                        "**Y√äU C·∫¶U:** Th·ª±c hi·ªán c√°c b∆∞·ªõc sau:\n"
                        "1. Di·ªÖn gi·∫£i v√† m·ªü r·ªông n·ªôi dung.\n"
                        "2. Li·ªát k√™ 5-10 t·ª´ kh√≥a.\n"
                        "3. ƒê·∫∑t ra 3 c√¢u h·ªèi gi·∫£ ƒë·ªãnh."
                    )
                    messages_batch.append([{"role": "user", "content": [{"type": "text", "text": prompt_text}]}])
                else:
                    messages_batch.append([{"role": "user", "content": f"Lo·∫°i chunk kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£."}])
            
            # G·ªçi model ƒë·ªÉ x·ª≠ l√Ω c·∫£ batch
            enriched_contents = self._generate_batch_response(messages_batch)

            # G√°n k·∫øt qu·∫£ tr·∫£ v·ªÅ cho c√°c chunk t∆∞∆°ng ·ª©ng
            for j, chunk in enumerate(batch_chunks):
                enriched_chunks_data.append({
                    'doc_name': chunk['doc_name'],
                    'chunk_type': chunk['chunk_type'],
                    'enriched_content': enriched_contents[j],
                    'metadata': chunk['metadata']
                })
        
        return enriched_chunks_data